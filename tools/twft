#!/bin/env python3

import argparse
import os
import json
from typing import Dict, List, Optional, Iterator
import concurrent.futures

import urllib.request
from urllib.parse import urlencode

FUZZ_KEY = 'FUZZME'

class Request:
    method: str
    route: str
    http_version: str  # NOTE(djnn): parsed but unsupported rn (idc tbh)

    body: Optional[Dict[str, str]]
    request_params: Optional[Dict[str, str]]
    headers: Dict[str, str]



def read_file_batches(file_path: str, batch_size: int = 10) -> Iterator[List[str]]:
    """Reads a text file in batches of lines."""
    with open(file_path, 'r') as file:
        lines = []

        for _ in range(batch_size):
            try:
                line = next(file)
                lines.append(line)

            # End of file reached, return the last batch if it's not full
            except StopIteration:
                if lines:
                    yield lines

                break
        else:
            yield lines


def parse_request_file(filepath: str) -> Optional[Request]:
    """parse the request file"""

    try:
        result = Request()

        with open(filepath, 'r') as file:
            # we could read line by line here, but we want to keep the body unified
            contents = file.read()
            headers_and_body = contents.split('\n\n')

            # assigning body if there is one
            assert len(headers_and_body) < 3
            result.body = json.loads(headers_and_body[1]) if len(headers_and_body) == 2 else None

            # splitting headers into lines
            headers = headers_and_body[0].split('\n')
            assert len(headers) != 0

            # parsing the first line (eg GET /?test=123 HTTP/1.1)
            first_line_split = headers[0].split(' ')
            assert len(first_line_split) == 3

            result.method = first_line_split[0]
            result.route = first_line_split[1]
            result.http_version = first_line_split[2]

            # split route after ?, if any items -> put them into the request params
            split_route = result.route.split('?')
            assert len(split_route) < 3

            # we have query params!
            if len(split_route) == 2:
                result.route = split_route[0]
                result.request_params = {}

                for param in split_route[1].split('&'):
                    keyval = param.split('=')
                    assert len(keyval) == 2
                    result.request_params[keyval[0]] = keyval[1]


            # parsing all headers :)
            for hdr_idx in range(1, len(headers)):
                header = headers[hdr_idx]
                keyval = header.split(': ')

                assert len(keyval) == 2
                result.headers[keyval[0]] = keyval[1]

            return result

    except (FileNotFoundError, PermissionError, AssertionError):
        return None


def run_request(replace_with: str, filter_by_size: int, use_https: bool, request: Request) -> None:
    """Function to be executed by each worker."""

    local_route = request.route.replace(FUZZ_KEY, replace_with)
    local_headers = {
        key.replace(FUZZ_KEY, replace_with): val.replace(FUZZ_KEY, replace_with)
             for key, val in request.headers.items()
    }
    local_params = {
     key.replace(FUZZ_KEY, replace_with): val.replace(FUZZ_KEY, replace_with)
             for key, val in request.request_params.items()
    } if request.request_params else {}


    scheme = 'https' if use_https else 'https'
    full_uri = f'{scheme}://{local_headers["Host"]}{local_route}'
    if local_params != {}:
        full_uri = f'{full_uri}?{urlencode(local_params)}'

    req = urllib.request.Request(full_uri, method=request.method)
    for key, val in local_headers.items():
        req.add_header(key, val)

    with urllib.request.urlopen(req) as f:
        contents = f.read().decode('utf-8')
        if len(contents) != filter_by_size:

            print(f'[+] Found interesting request: {full_uri}')
            print('----====[ REQUEST HEADERS ]====----')
            for key, val in local_headers.items():
                print(f'\t{key}: {val}')
            print(f'\n\nStatus: {f.status}\n')
            print('----====[ RESPONSE ]====----')
            print(f'{contents}\n----------------------------\n')


def main() -> None:

    # set up argument parser and command-line options
    parser = argparse.ArgumentParser(description="tiny web fuzzing tool")
    parser.add_argument("-s", "--https", action="store_true", help="force https usage", default=False)
    parser.add_argument("-w", "--wordlist", help="wordlist filepath", default="common.txt")
    parser.add_argument("-fs", "--filter-size", help="filter by size", type=int, default=0)
    parser.add_argument("request", help="filepath containing the HTTP request definition")

    parser.parse_args()

    if not parser.request or not parser.wordlist:   # type: ignore
        print("[!] error: missing request or wordlist parameter")
        print("Please run --help option!")
        os._exit(1)

    request_items = parse_request_file(parser.request)  # type: ignore
    if not request_items:
        print('[!] error: could not read from file.')
        os._exit(1)

    filter_size = int(parser.filter_size)  # type: ignore
    use_https = bool(parser.https if parser.https else False)  # type: ignore
    batch_iterator = read_file_batches(parser.wordlist)  # type: ignore
    with concurrent.futures.ThreadPoolExecutor() as executor:
        while True:
            try:
                batch = next(batch_iterator)
                if not batch:
                    break

                futures = [executor.submit(run_request, line, filter_size, use_https, request_items) for line in batch]
                concurrent.futures.wait(futures)

            except StopIteration:
                break


if __name__ == '__main__':
    main()
